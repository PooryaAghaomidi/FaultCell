{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Set base configurations"
   ],
   "metadata": {
    "id": "nHEZxo65A3-i"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FGW4-7N7A0aZ"
   },
   "source": [
    "seed_value= 42\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "id": "iomiI7SAC1a-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "id": "vOWo15wYBUzb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Conv2D, Flatten, UpSampling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, BatchNormalization, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Add, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers, optimizers, losses\n",
    "from livelossplot.inputs.tf_keras import PlotLossesCallback"
   ],
   "metadata": {
    "id": "fCETuUYQBYWA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "id": "Gf7wd3EQChEx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs  = 200\n",
    "batch_size  = 32\n",
    "cls_num     = 8\n",
    "shape       = (1000, 13)\n",
    "lr          = 0.0001\n",
    "opt         = optimizers.Adam(learning_rate=lr)\n",
    "los         = losses.BinaryCrossentropy()\n",
    "mtr         = ['accuracy']"
   ],
   "metadata": {
    "id": "2HBks-vyCjwu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "address = '../Dataset/Main-1000'\n",
    "\n",
    "files = os.listdir(address)\n",
    "random.shuffle(files)\n",
    "l = len(files)\n",
    "\n",
    "data_train = files[:int(0.8*l)]\n",
    "data_test = files[int(0.8*l):int(0.9*l)]\n",
    "data_val = files[int(0.9*l):]\n",
    "\n",
    "steps_per_train = len(data_train)//batch_size\n",
    "steps_per_test = len(data_test)//batch_size\n",
    "steps_per_val = len(data_val)//batch_size"
   ],
   "metadata": {
    "id": "DgCEa8LBE8Cm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data generator"
   ],
   "metadata": {
    "id": "DMjId1dv9WVQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, address, data, shape, batch_size, cls_num, shuffle=True):\n",
    "        self.address    = address\n",
    "        self.data       = data\n",
    "        self.shape      = shape\n",
    "        self.batch_size = batch_size\n",
    "        self.cls_num    = cls_num\n",
    "        self.shuffle    = shuffle\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes       = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.data[k] for k in indexes]\n",
    "        x, y          = self.__data_generation(list_IDs_temp)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)        \n",
    "        \n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        x = np.empty((self.batch_size, int(self.shape[0]), int(self.shape[1])))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            raw_data = np.load(os.path.join(self.address, ID))\n",
    "            sig = np.transpose(raw_data['signals'])\n",
    "            \n",
    "            for idx, row in enumerate(sig):\n",
    "                x[i, :, idx] = (row - row.min()) / (row.max() - row.min())\n",
    "                \n",
    "            y[i]  = int(str(raw_data['label'])[0])\n",
    "\n",
    "        return x, to_categorical(y, num_classes=self.cls_num)\n",
    "\n",
    "\n",
    "train_gen = DataGenerator(address, data_train, shape, batch_size, cls_num)\n",
    "test_gen  = DataGenerator(address, data_test , shape, batch_size, cls_num)\n",
    "val_gen   = DataGenerator(address, data_val  , shape, batch_size, cls_num)"
   ],
   "metadata": {
    "id": "Lqfej_YmBcp0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sample = next(iter(test_gen))\n",
    "x_sample = test_sample[0]\n",
    "y_sample = test_sample[1]\n",
    "\n",
    "print('Label: ', y_sample[0])\n",
    "print('Signal max: ', x_sample[1].max())\n",
    "print('Signal min: ', x_sample[1].min())\n",
    "print('Single signal shape: ', x_sample[1][:, 0].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "fFEJYG-GBdI-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model(myshape, cls_num):\n",
    "  inputs = Input(shape=myshape)\n",
    "  \n",
    "  x = Conv1D(filters=128, kernel_size=8, strides=1, activation='relu')(inputs)\n",
    "  x = MaxPooling1D(2)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  \n",
    "  x = Conv1D(filters=128, kernel_size=8, strides=1, activation='relu')(x)\n",
    "  x = MaxPooling1D(2)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  \n",
    "  temporal_1 = GRU(128, return_sequences=True)(x)\n",
    "  temporal_1 = GRU(64, return_sequences=False)(temporal_1)\n",
    "  \n",
    "  temporal_2 = GRU(64, return_sequences=False)(x)\n",
    "  \n",
    "  adding = Add()([temporal_1, temporal_2])\n",
    "  adding = ReLU()(adding)\n",
    "  adding = Dropout(0.2)(adding)\n",
    "  adding = Flatten()(adding)\n",
    "  \n",
    "  outputs = Dense(cls_num, activation='softmax')(adding)\n",
    "\n",
    "  return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = get_model(shape, cls_num)\n",
    "model.compile(optimizer=opt, loss=los, metrics=mtr)\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "Epe7Q_oqBeEr"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "lzs7DO9MBenF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def callback():\n",
    "  mymonitor = 'val_loss'\n",
    "  mymode    = 'min'\n",
    "\n",
    "  main_chk  = ModelCheckpoint(filepath='checkpoints/base', monitor=mymonitor, mode=mymode, verbose=1, save_best_only=True)\n",
    "  early_st  = EarlyStopping(monitor=mymonitor, mode=mymode, patience=10, verbose=1)\n",
    "  rduce_lr  = ReduceLROnPlateau(monitor=mymonitor, mode=mymode, factor=0.5, patience=5, verbose=1, min_lr=0.00001)\n",
    "  tr_plot   = PlotLossesCallback()\n",
    "\n",
    "  return [main_chk, early_st, rduce_lr, tr_plot]\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callback(),\n",
    "                    steps_per_epoch  = steps_per_train,\n",
    "                    validation_steps = steps_per_val)"
   ],
   "metadata": {
    "id": "v-XEoecuBfmN"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "id": "WnhMRDgNBgIN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "testmodel = load_model('checkpoints/base', compile=True)\n",
    "tst_loss , tst_acc = testmodel.evaluate(test_gen, steps = steps_per_test)"
   ],
   "metadata": {
    "id": "h2ceCOoJBicL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
